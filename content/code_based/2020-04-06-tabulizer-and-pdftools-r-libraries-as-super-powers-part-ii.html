---
title: Tabulizer and pdftools R Libraries as Super-powers - Part II
author: David Lucey
date: '2020-04-06'
slug: tabulizer-and-pdftools-r-libraries-as-super-powers-part-ii
categories:
  - R
  - XBRL
  - Tabula
  - pdftools
tags:
  - pdf
  - rstats
  - ctdata
---



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>When Redwall discovered pdftools, and its pdf_data function, which maps out every word on a pdf page by x-y coordinate, we thought that was interesting, but didn’t really know how to use it. We also didn’t have the regular expression skills, and were much more befuddled by the nested list structures than we are now.</p>
<p>As for Tabulizer, it took about a year before rJava magically started working properly, but even then we it wasn’t possible to consistently read a large number of tables of different sizes without cutting off fields in unexpected ways. Only in this Mass pdf scraping project have we realized that by combining these two packages, it becomes possible to access data with fairly reliable way from a large amount of varied pdf formats.</p>
<p>Our Massachusset’s municipal CAFR project provided a perfect opportunity to put all these pieces together. This blog post will consist of a step-by-step walk through which will hopefully help others avoid some of the pain that we experienced in getting to this point.</p>
</div>
<div id="walk-through-plan" class="section level1">
<h1>Walk Through Plan</h1>
<p>To begin with, we had to download the pdfs from the Center for Municipal Finance. We won’t show the code to do this and all of this project here, but it can be found at <a href="https://github.com/luceydav/pdf_cafr_parse/blob/master/reason_pdf_parser.R">reason_pdf_parser.R</a>. In order to do this on the scale that we did for the project, we had to build large nested lists of all 150 Massachussett’s CAFR pdfs. For now, we will just walk through a few key points on the Abington 2018 Statement of Net Position. The 2018 Abington CAFR used in this example is available for download <a href="https://www.abingtonpa.gov/departments/finance/cafr">here</a>.</p>
<pre class="r"><code># Set up pdf and pdf_path to directory
dir &lt;- &quot;/Users/davidlucey/Desktop/David/Projects/mass_munis/data/pdf_cafr/&quot;
city &lt;- &quot;abington&quot;
pdf &lt;- paste0(city, &quot;_2018.pdf&quot;, collapse=&quot;&quot;)
pdf_path &lt;- paste0(dir, pdf, collapse = &quot;&quot;)

# Run pdf_data on Abington CAFR
abington &lt;- pdftools::pdf_data(pdf_path)

# Name each page of list for page index in pdf
names(abington) &lt;- 1:length(abington)

# Look at structure of 1st two elements of nested list
str(abington[1:2])</code></pre>
<pre><code>## List of 2
##  $ 1:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:   16 obs. of  6 variables:
##   ..$ width : int [1:16] 36 16 67 107 50 18 84 16 37 64 ...
##   ..$ height: int [1:16] 11 11 11 11 11 11 11 11 11 11 ...
##   ..$ x     : int [1:16] 281 321 341 411 311 364 385 473 305 346 ...
##   ..$ y     : int [1:16] 323 323 323 323 368 368 368 368 384 384 ...
##   ..$ space : logi [1:16] TRUE TRUE TRUE FALSE TRUE TRUE ...
##   ..$ text  : chr [1:16] &quot;TOWN&quot; &quot;OF&quot; &quot;ABINGTON,&quot; &quot;MASSACHUSETTS&quot; ...
##  $ 2:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:   266 obs. of  6 variables:
##   ..$ width : int [1:266] 42 19 76 124 58 20 96 19 41 73 ...
##   ..$ height: int [1:266] 15 15 15 15 15 15 15 15 15 15 ...
##   ..$ x     : int [1:266] 168 215 238 319 92 154 179 279 302 348 ...
##   ..$ y     : int [1:266] 72 72 72 72 102 102 102 102 102 102 ...
##   ..$ space : logi [1:266] TRUE TRUE TRUE FALSE TRUE TRUE ...
##   ..$ text  : chr [1:266] &quot;TOWN&quot; &quot;OF&quot; &quot;ABINGTON,&quot; &quot;MASSACHUSETTS&quot; ...</code></pre>
</div>
<div id="pdf-tools-pdf_data-functionality" class="section level1">
<h1>PDF Tools pdf_data Functionality</h1>
<p>The above is a list of data.frames containing metadata of the location of every word on every one of the 92 pages of the Abington 2018 CAFR. For example, the first page contains 16 and the second 266 words. But, we only need the key financial statements. So, it is possible to drop pages which don’t have what we need. We know that page 16 has the Statement of net position. So we could search for that page like as shown for variable “sonp” below. We use the first 5 lines to look for the phrase “STATEMENT OF NET POSITION”, but the “Proprietary Funds” and some other pages often have this phrase. Most of these other pages can be eliminated by choosing not to match the word “FUNDS”.</p>
<pre class="r"><code># Convert elements to data.table
abington &lt;- lapply(abington, setDT)

# Get index of Abington Statement of Net Position
sonp_index &lt;- 
  which(
    unlist(
      lapply(abington, function(page){
        (str_detect(
          paste(
            # Reformat top 5 lines by y and look for match to &quot;STATEMENT OF NET POSITION&quot;
              page$text[
                page$y %in% head(unique(page$y), 5)
                ],
              collapse = &quot; &quot;
              ),
          &quot;STATEMENT OF NET POSITION&quot;
        ) &amp; 
          # And requires both statements to be TRUE
          !str_detect(
            paste(
              page$text[
               # Reformat top 5 lines by y and look for non match to &quot;FUNDS&quot; 
                page$y %in% head(unique(page$y), 5)
                ],
              collapse = &quot; &quot;
              ),
            &quot;FUNDS&quot;
            )
        )
        }
        )
    )
    )
    
# Extract and View Statement of Net Position pdftools pdf_data  metadata
sonp &lt;- abington[sonp_index][[1]]
sonp</code></pre>
<pre><code>##      width height   x   y space          text
##   1:    32      6 264  73  TRUE     STATEMENT
##   2:     7      6 299  73  TRUE            OF
##   3:    10      6 308  73  TRUE           NET
##   4:    25      6 320  73 FALSE      POSITION
##   5:    14      6 287  87  TRUE          JUNE
##  ---                                         
## 337:    71      9 144 745 FALSE Massachusetts
## 338:    11     11 300 743 FALSE            13
## 339:    26      9 426 745  TRUE         Basic
## 340:    43      9 456 745  TRUE     Financial
## 341:    53      9 502 745 FALSE    Statements</code></pre>
<p>Above is the text grid of Abington’s Statement of Net Position as taken by pdftools’ pdf_data function. Now we can begin to put together area parameters for Tabula. Using this grid, it is possible to extact the x-y coordinates of the four corners of the table, which allows a user to exactly specify area coordinates for Tabulizer. In our experience, this is important because Tabulizer’s default “lattice” method for tabular data can be unpredictable cutting off fields unexpectedly.</p>
</div>
<div id="tabulizer-area-coordinates" class="section level1">
<h1>Tabulizer Area Coordinates</h1>
<p>Tabulizer specifies pages in blocks of 72 * inches, so a typical page would have dimensions of 612 x 720. All of of Massachusett’s pdfs have a $ sign in the first and last rows, so that could be the top. In addition, all pages including financial statements have language referring users to the notes to the financial statements at the bottom.</p>
<p>We give an example for Abington’s Statement of Net Position below starting with the maximum “x” and “y”, and determining the orientation. We then find the location of the date line at the top and walk down a little from there to set a table top. Typically, it is best to leave a little margin between the page header and the top of the table. The bottom of the table is set adding the height to the bottom line of the table, and left parameter is set by taking the smallest “x” coordinate and reducing by a little to avoid mistakes. We leave an larger margin for the right-most coordinate this is where a lot of errors can occur if the table algorithm tries to squish the table.</p>
<p>In our experience, the most problems come with missetting the top and right parameters. Columns can be split in the middle into two columns, often at the far-rightmost, for example. In the end, we chose parameters of 93 (top), 24 (left), 681 (bottom) and 585 (right).</p>
<pre class="r"><code> # Determine if page is verticle or horizontal
    x &lt;- 8.5 * 72
    y &lt;- 11 * 72
    max_x &lt;- max(sonp$x)
    max_y &lt;- max(sonp$y)
    orientation &lt;- 
      ifelse(x &lt; max_x, &quot;horizontal&quot;, &quot;verticle&quot;)
    
    # TOP
    
    # Keys on the first instance of the year &quot;2018&quot;
    table_top &lt;-
      min(sonp$y[str_detect(sonp$text, &quot;2018&quot;) &amp; sonp$space==FALSE])
    # Find the height at in the table_top row
    height_top &lt;- unique(sonp$height[sonp$y == table_top])
    # Add table_top and height_top to avoid slicing row
    top &lt;- table_top + height_top 
    
    # BOTTOM
    
    # Table Bottom marked by last instance of character &quot;$&quot;
    table_bottom &lt;-
      max(sonp$y[str_detect(sonp$text, &quot;\\$&quot;)])
    # Height at bottom row of table 
    height_bottom &lt;- unique(sonp$height[sonp$y == table_bottom])
    # Bottom of table
    bottom &lt;- table_bottom + height_bottom
    
    # LEFT
    
    # Add some space to leftmost x coordinate to avoid slicing
    left &lt;-     
      ifelse( min(sonp$x) - 30 &gt; 0,
              min(sonp$x) - 30, 1 )
    
    # RIGHT
    
    # Find width at maximum &quot;x&quot; coordinate
    width_max_x &lt;- max(sonp$width[sonp$x == max_x])
    # Add width at maximum &quot;x&quot; plus more space wether verticle or horizontal
    right &lt;- 
      max_x + width_max_x + ifelse(orientation == &quot;verticle&quot;, 30, 50)
    
    # FINAL AREA PARAMETER FOR TABULIZER AS INTEGER VECTOR
    # Note the specification as an integer vector
    a &lt;- c(top, left, bottom, right)
    
    # Show coordinates 
    a</code></pre>
<pre><code>## [1]  93  24 681 585</code></pre>
</div>
<div id="tabulizer-extract_table-function" class="section level1">
<h1>Tabulizer extract_table Function</h1>
<p>Below we run our parameters from above through Tabulizer. Note that the area parameter, itself an integer vector, is further wrapped as a list because not having this structure throws an error. In addition, please avoid the half day of wheel spinning we experienced by specifying guess as “F” to over-ride the default lattice, otherwise your area parameter is ignored with no warning. Also, we use the sonp_index integer to specify the page of the pdf. There are several options for output which all work as expected, but data.frame seems most natural.</p>
<pre class="r"><code># Tabulizer extract_tables output is a list
abington_sonp &lt;-
  extract_tables(
    pdf_path, 
    pages = sonp_index,
    area = list(a), 
    guess = F,
    output = &quot;data.frame&quot;)

# Extract single element from list
abington_sonp &lt;- abington_sonp[[1]]</code></pre>
</div>
<div id="clean-up" class="section level1">
<h1>Clean up</h1>
<p>The output is still in a raw form with colums sometimes determined by indentations and “x” values, such as the “$” signs. The numbers are in character form with commas, and need to be parsed into numeric. The biggest challenge is the column names which often include the first row of the full column name, and need to be rebuilt. This is not a small task and not what we were hoping to illustrate in this post, so we are just showing the output below. Please refer to our Github code for the a more complete explanation.</p>
<pre class="r"><code>abington_final &lt;- clean_table(abington_sonp)

abington_final</code></pre>
</div>
<div id="final-product" class="section level1">
<h1>Final Product</h1>
<p>Though there is still work to be done, the final product of this post is shown below. Single elements could be extracted to form a database, or the output could be saved to csv. The headers such as ASSETS AAND LIABILITIES could be nested. The main point is that short of XBRL, the data has been set free from the PDF in a machine readable form. Not only that, this general process can be repeated for a large number of slightly differing PDFs with a relatively high low error rate. In cases where errors do occur, a second layer can be used to run the more challenging PDFs through AWS Textract SDK. We will show how this is done in our next post.</p>
</div>
